# PrzykÅ‚ady UÅ¼ycia

## PrzykÅ‚ad 1: Podstawowy Crawl

### Komenda
```bash
node index.js https://example.com
```

### Output
```
ğŸ•·ï¸  Starting recursive crawl from: https://example.com
ğŸ’¾ Live saving enabled (every 5 URLs)

ğŸ“¡ Crawling... Found: 47 URLs | Queue: 3 | Processed: 45

âœ“ Crawl complete! Found 47 URLs total.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                      CRAWL RESULTS                            
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total URLs found: 47

Results grouped by content-type:

ğŸ“„ text/html (12 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://example.com
  2. https://example.com/about
  3. https://example.com/contact
  ...

ğŸ“„ image/jpeg (15 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://example.com/images/photo1.jpg
  2. https://example.com/images/photo2.jpg
  ...

ğŸ“„ text/css (8 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://example.com/css/style.css
  2. https://example.com/css/theme.css
  ...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ All results saved to results.json
```

---

## PrzykÅ‚ad 2: Crawl z Resume (Przerwany)

### Komenda - Pierwsza prÃ³ba
```bash
node index.js https://large-site.com -o large.json -s 10
```

### Output (przerwany po 50 URLach)
```
ğŸ•·ï¸  Starting recursive crawl from: https://large-site.com
ğŸ’¾ Live saving enabled (every 10 URLs)

ğŸ“¡ Crawling... Found: 50 URLs | Queue: 127 | Processed: 48
^C
âœ— Error during crawl: ...

ğŸ’¾ Progress saved before exit (50 URLs)
```

### Komenda - Wznowienie
```bash
node index.js https://large-site.com --resume -o large.json
```

### Output - Resume
```
ğŸ•·ï¸  Starting recursive crawl from: https://large-site.com
ğŸ”„ Attempting to resume from previous session...
âœ“ Found previous session with 50 URLs
âœ“ Resuming with 127 URLs in queue
ğŸ’¾ Live saving enabled (every 5 URLs)

ğŸ“¡ Crawling... Found: 234 URLs | Queue: 0 | Processed: 232

âœ“ Crawl complete! Found 234 URLs total.
```

---

## PrzykÅ‚ad 3: Filtrowanie po Content-Type

### Komenda - Tylko obrazy
```bash
node index.js https://photo-site.com --filter "image/" -o images.json
```

### Output
```
ğŸ•·ï¸  Starting recursive crawl from: https://photo-site.com
ğŸ’¾ Live saving enabled (every 5 URLs)

ğŸ“¡ Crawling... Found: 156 URLs | Queue: 0 | Processed: 154

âœ“ Crawl complete! Found 156 URLs total.

ğŸ” Filtered by "image/": 89/156 URLs

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                      CRAWL RESULTS                            
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total URLs found: 89

Results grouped by content-type:

ğŸ“„ image/jpeg (45 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://photo-site.com/gallery/photo1.jpg
  2. https://photo-site.com/gallery/photo2.jpg
  ...

ğŸ“„ image/png (32 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://photo-site.com/icons/icon1.png
  2. https://photo-site.com/icons/icon2.png
  ...

ğŸ“„ image/gif (12 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://photo-site.com/animations/anim1.gif
  ...
```

---

## PrzykÅ‚ad 4: Limit GÅ‚Ä™bokoÅ›ci

### Komenda
```bash
node index.js https://deep-site.com --max-depth 2 -o shallow.json
```

### Output
```
ğŸ•·ï¸  Starting recursive crawl from: https://deep-site.com
ğŸ’¾ Live saving enabled (every 5 URLs)

ğŸ“¡ Crawling... Found: 23 URLs | Queue: 0 | Processed: 23

âœ“ Crawl complete! Found 23 URLs total.

Total URLs found: 23
```

**Uwaga:** Crawler zatrzymaÅ‚ siÄ™ na gÅ‚Ä™bokoÅ›ci 2 (strona gÅ‚Ã³wna â†’ linki â†’ linki z tych linkÃ³w)

---

## PrzykÅ‚ad 5: Niestandardowy Interval Zapisu

### Komenda
```bash
node index.js https://example.com --save-interval 3 -o frequent.json
```

### Output
```
ğŸ•·ï¸  Starting recursive crawl from: https://example.com
ğŸ’¾ Live saving enabled (every 3 URLs)

ğŸ“¡ Crawling... Found: 47 URLs | Queue: 3 | Processed: 45

âœ“ Crawl complete! Found 47 URLs total.
```

**Rezultat:** Plik `frequent.json` byÅ‚ aktualizowany co 3 znalezione URLe zamiast domyÅ›lnych 5.

---

## PrzykÅ‚ad 6: ZÅ‚oÅ¼ona Komenda

### Komenda
```bash
node index.js https://complex-site.com \
  --max-depth 4 \
  --save-interval 15 \
  --output complex-crawl.json \
  --filter "text/html"
```

### Output
```
ğŸ•·ï¸  Starting recursive crawl from: https://complex-site.com
ğŸ’¾ Live saving enabled (every 15 URLs)

ğŸ“¡ Crawling... Found: 312 URLs | Queue: 0 | Processed: 310

âœ“ Crawl complete! Found 312 URLs total.

ğŸ” Filtered by "text/html": 87/312 URLs

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                      CRAWL RESULTS                            
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total URLs found: 87

Results grouped by content-type:

ğŸ“„ text/html (87 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://complex-site.com
  2. https://complex-site.com/page1
  ...
```

---

## PrzykÅ‚ad 7: Plik Wynikowy (results.json)

```json
{
  "crawledAt": "2025-10-08T14:30:00.000Z",
  "lastUpdated": "2025-10-08T14:35:22.000Z",
  "startUrl": "https://example.com",
  "maxDepth": "unlimited",
  "totalUrls": 47,
  "inProgress": false,
  "completedAt": "2025-10-08T14:35:22.000Z",
  "allResults": [
    {
      "url": "https://example.com",
      "contentType": "text/html"
    },
    {
      "url": "https://example.com/style.css",
      "contentType": "text/css"
    },
    {
      "url": "https://example.com/image.jpg",
      "contentType": "image/jpeg"
    },
    {
      "url": "https://example.com/about",
      "contentType": "text/html"
    }
  ]
}
```

---

## PrzykÅ‚ad 8: Error Handling

### Komenda
```bash
node index.js https://unreliable-site.com -o errors.json
```

### Output (niektÃ³re URLe failujÄ…)
```
ğŸ•·ï¸  Starting recursive crawl from: https://unreliable-site.com
ğŸ’¾ Live saving enabled (every 5 URLs)

ğŸ“¡ Crawling... Found: 28 URLs | Queue: 5 | Processed: 26

âœ“ Crawl complete! Found 28 URLs total.

Total URLs found: 28

Results grouped by content-type:

ğŸ“„ text/html (15 URLs)
...

ğŸ“„ error (3 URLs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. https://unreliable-site.com/broken-link
  2. https://unreliable-site.com/timeout-page
  3. https://unreliable-site.com/404-not-found
```

### W pliku JSON
```json
{
  "allResults": [
    {
      "url": "https://unreliable-site.com/broken-link",
      "contentType": "error"
    }
  ]
}
```

---

## PrzykÅ‚ad 9: Live Monitoring

### Terminal 1 - Crawling
```bash
node index.js https://huge-site.com -o huge.json -s 20
```

### Terminal 2 - Monitoring
```bash
# Obserwuj rozmiar pliku
watch -n 2 "ls -lh huge.json"

# Liczenie URLi
watch -n 2 "cat huge.json | grep -o '\"url\"' | wc -l"

# SprawdÅº czy completed
watch -n 5 "cat huge.json | grep 'inProgress'"
```

---

## PrzykÅ‚ad 10: Help

### Komenda
```bash
node index.js --help
```

### Output
```
Web Crawler CLI - Recursively discover same-domain links and their content-types

Usage:
  node index.js <URL> [options]

Arguments:
  <URL>                    Starting URL to crawl (required)

Options:
  -f, --filter <type>      Filter results by content-type (e.g., "text/html", "image/")
  -o, --output <file>      Output file name (default: results.json)
  -d, --max-depth <num>    Maximum crawl depth (default: unlimited)
  -s, --save-interval <n>  Save progress every N URLs (default: 5)
  -r, --resume             Resume from previous crawl session
  -h, --help               Show this help message

Examples:
  node index.js https://example.com
  node index.js https://example.com --filter "text/html"
  node index.js https://example.com --output my-results.json
  node index.js https://example.com --max-depth 3
  node index.js https://example.com --save-interval 10
  node index.js https://example.com --resume
  node index.js https://example.com -f "image/" -o images.json -d 2 -s 3
```

---

## WskazÃ³wki

1. **Zawsze uÅ¼ywaj `-o` dla nazwanych projektÃ³w**
   - Åatwiej zarzÄ…dzaÄ‡ wieloma crawlami
   - MoÅ¼esz wznowiÄ‡ konkretny crawl

2. **Adjust save-interval dla wydajnoÅ›ci**
   - MaÅ‚e wartoÅ›ci (1-5): Bezpieczniejsze, wiÄ™cej zapisÃ³w
   - DuÅ¼e wartoÅ›ci (20-50): Szybsze, mniej operacji I/O

3. **UÅ¼yj --max-depth dla eksploracji**
   - Najpierw przetestuj z `-d 2`
   - Potem zwiÄ™ksz jeÅ›li potrzeba

4. **Resume jest Twoim przyjacielem**
   - MoÅ¼esz przerwaÄ‡ w kaÅ¼dej chwili (Ctrl+C)
   - WznÃ³w pÃ³Åºniej bez utraty danych

5. **Filter na koÅ„cu**
   - PeÅ‚ny crawl zapisuje wszystko
   - Filter tylko zmienia wyÅ›wietlanie
   - MoÅ¼esz przefiltrowaÄ‡ JSON pÃ³Åºniej narzÄ™dziami jak `jq`

