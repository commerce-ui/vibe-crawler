# Live Save & Resume - Przewodnik

## ğŸ¯ Po co Live Saving?

Live saving chroni Twoje dane przed utratÄ… w przypadku:
- âŒ Awarii programu
- âŒ BÅ‚Ä™du sieciowego
- âŒ Przerwania (Ctrl+C)
- âŒ Awarii systemu
- âŒ Timeout na duÅ¼ych stronach

## ğŸ’¾ Jak dziaÅ‚a Live Saving?

Crawler automatycznie zapisuje postÄ™p **co 5 znalezionych URLi** (domyÅ›lnie).

```bash
# Podstawowe uÅ¼ycie - automatyczny zapis co 5 URLi
node index.js https://example.com

# CzÄ™stszy zapis - co 3 URLi (bezpieczniejsze)
node index.js https://example.com --save-interval 3

# Rzadszy zapis - co 20 URLi (szybsze, mniej operacji I/O)
node index.js https://example.com --save-interval 20
```

### Co jest zapisywane?

Plik JSON zawiera:
- âœ… Wszystkie znalezione URLe z content-type
- âœ… Metadata (data rozpoczÄ™cia, URL startowy, max depth)
- âœ… Stan kolejki (pierwsze 100 URLi do odwiedzenia)
- âœ… Status crawlowania (`inProgress: true/false`)
- âœ… Informacje o bÅ‚Ä™dach (jeÅ›li wystÄ…piÅ‚y)

## ğŸ”„ Wznawianie Crawlowania

### Scenariusz 1: Crawler siÄ™ zawiesiÅ‚

```bash
# Pierwsze uruchomienie
node index.js https://example.com -o mycrawl.json

# ... crawler siÄ™ zawiesza po 50 URLach ...

# WznÃ³w od miejsca przerwania
node index.js https://example.com --resume -o mycrawl.json
```

### Scenariusz 2: Celowe przerwanie (Ctrl+C)

```bash
# Zacznij crawlowaÄ‡
node index.js https://duza-strona.com -o duza.json

# NaciÅ›nij Ctrl+C aby przerwaÄ‡...

# PÃ³Åºniej wrÃ³Ä‡ i dokoÅ„cz
node index.js https://duza-strona.com --resume -o duza.json
```

### Scenariusz 3: Testowanie z resume

```bash
# Crawluj tylko 2 poziomy, zapisuj czÄ™sto
node index.js https://example.com -d 2 -s 3 -o test.json

# SprawdÅº wyniki, potem crawluj gÅ‚Ä™biej
node index.js https://example.com -d 5 --resume -o test.json
```

## ğŸ“Š Monitorowanie PostÄ™pu

MoÅ¼esz oglÄ…daÄ‡ plik wynikowy w czasie rzeczywistym:

```bash
# Terminal 1: Uruchom crawlera
node index.js https://example.com -o results.json

# Terminal 2: Obserwuj postÄ™p
watch -n 2 "cat results.json | grep -o '\"url\"' | wc -l"

# Lub po prostu otwÃ³rz results.json w edytorze
```

## âš™ï¸ Zaawansowane UÅ¼ycie

### Bardzo duÅ¼a strona, maksymalne bezpieczeÅ„stwo

```bash
# Zapisuj co 1 URL, bez limitu gÅ‚Ä™bokoÅ›ci
node index.js https://ogromna-strona.com -s 1 -o ogromna.json
```

### Szybki crawl z okazjonalnym zapisem

```bash
# Zapisuj co 50 URLi, max 3 poziomy
node index.js https://example.com -s 50 -d 3 -o szybki.json
```

### Crawl tylko obrazÃ³w z resumem

```bash
# Pierwszy run
node index.js https://example.com -f "image/" -o obrazy.json

# JeÅ›li przerwane, wznÃ³w
node index.js https://example.com --resume -o obrazy.json

# Uwaga: filter dziaÅ‚a tylko na wyÅ›wietlaniu, plik ma wszystkie URLe
```

## ğŸ” Sprawdzanie Stanu Pliku

Plik wynikowy zawiera pole `inProgress`:
- `"inProgress": true` - Crawl byÅ‚ przerwany
- `"inProgress": false` - Crawl zakoÅ„czony prawidÅ‚owo

```bash
# SprawdÅº czy crawl jest kompletny
cat results.json | grep "inProgress"

# SprawdÅº ile URLi znaleziono
cat results.json | grep "totalUrls"

# SprawdÅº czy byÅ‚y bÅ‚Ä™dy
cat results.json | grep "error"
```

## ğŸ’¡ Najlepsze Praktyki

1. **Dla maÅ‚ych stron** (< 100 stron):
   ```bash
   node index.js https://small-site.com
   # DomyÅ›lne ustawienia sÄ… OK
   ```

2. **Dla Å›rednich stron** (100-1000 stron):
   ```bash
   node index.js https://medium-site.com -s 10 -d 5
   # Zapisuj co 10 URLi, max 5 poziomÃ³w
   ```

3. **Dla duÅ¼ych stron** (1000+ stron):
   ```bash
   node index.js https://huge-site.com -s 5 -d 3 -o huge.json
   # CzÄ™ste zapisy, ogranicz gÅ‚Ä™bokoÅ›Ä‡
   # MoÅ¼esz przerwaÄ‡ i wznowiÄ‡ w dowolnym momencie
   ```

4. **Dla niestabilnej sieci**:
   ```bash
   node index.js https://example.com -s 3
   # Bardzo czÄ™ste zapisy
   ```

## ğŸš¨ Troubleshooting

### Problem: Resume nie dziaÅ‚a

**SprawdÅº:**
1. Czy uÅ¼ywasz tego samego pliku output (`-o`)
2. Czy URL startowy jest identyczny
3. Czy plik JSON nie jest uszkodzony

```bash
# Waliduj JSON
cat results.json | python -m json.tool > /dev/null
echo $?  # Powinno byÄ‡ 0
```

### Problem: Zbyt duÅ¼y plik JSON

**RozwiÄ…zanie:** ZwiÄ™ksz save-interval

```bash
# Zamiast co 5 URLi, zapisuj co 50
node index.js https://example.com -s 50
```

### Problem: ChcÄ™ zaczÄ…Ä‡ od nowa

**RozwiÄ…zanie:** Po prostu usuÅ„ stary plik

```bash
rm results.json
node index.js https://example.com
```

## ğŸ“ Format Pliku State

```json
{
  "crawledAt": "2025-10-08T12:00:00.000Z",
  "lastUpdated": "2025-10-08T12:05:30.000Z",
  "startUrl": "https://example.com",
  "maxDepth": "unlimited",
  "totalUrls": 47,
  "inProgress": true,
  "queue": [
    { "url": "https://example.com/page", "depth": 1 }
  ],
  "allResults": [
    { "url": "https://example.com", "contentType": "text/html" },
    { "url": "https://example.com/style.css", "contentType": "text/css" }
  ]
}
```

## ğŸ‰ Podsumowanie

Live saving i resume to kluczowe funkcje dla:
- âœ… BezpieczeÅ„stwa danych
- âœ… DÅ‚ugotrwaÅ‚ych crawlÃ³w
- âœ… Eksperymentowania (start-stop-resume)
- âœ… Monitorowania postÄ™pu
- âœ… Odzyskiwania po bÅ‚Ä™dach

**Crawluj bez obaw - Twoje dane sÄ… bezpieczne!** ğŸ’ª

