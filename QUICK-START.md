# Quick Start Guide

## Instalacja

```bash
# 1. Zainstaluj zale≈ºno≈õci
npm install

# 2. Gotowe do u≈ºycia!
```

## Podstawowe Przyk≈Çady

### 1. Najprostszy crawl
```bash
node index.js https://example.com
```
- Crawluje ca≈ÇƒÖ domenƒô rekursywnie
- Zapisuje co 5 URLi do `results.json`
- Bezpieczne - nie stracisz postƒôpu

### 2. Z limitem g≈Çƒôboko≈õci
```bash
node index.js https://example.com --max-depth 3
```
- Crawluje tylko 3 poziomy w g≈ÇƒÖb
- Szybsze dla du≈ºych stron

### 3. Znajd≈∫ konkretny typ contentu
```bash
# Wszystkie obrazy
node index.js https://example.com --filter "image/"

# Tylko HTML strony
node index.js https://example.com --filter "text/html"

# Pliki CSS
node index.js https://example.com --filter "text/css"
```

### 4. Bezpieczny crawl z wznawianiem
```bash
# Rozpocznij crawl
node index.js https://duza-strona.com -o mycrawl.json

# Je≈õli siƒô przerwie (b≈ÇƒÖd, Ctrl+C), wzn√≥w:
node index.js https://duza-strona.com --resume -o mycrawl.json
```

### 5. Pe≈Çna kontrola
```bash
node index.js https://example.com \
  --max-depth 5 \
  --save-interval 10 \
  --output my-results.json \
  --filter "text/html"
```

## Skr√≥ty Opcji

| Pe≈Çna nazwa | Skr√≥t | Przyk≈Çad |
|-------------|-------|----------|
| `--filter` | `-f` | `-f "image/"` |
| `--output` | `-o` | `-o out.json` |
| `--max-depth` | `-d` | `-d 3` |
| `--save-interval` | `-s` | `-s 10` |
| `--concurrency` | `-c` | `-c 10` |
| `--resume` | `-r` | `-r` |
| `--help` | `-h` | `-h` |

## Czƒôste Scenariusze

### Ma≈Ça strona (< 100 stron)
```bash
node index.js https://small-site.com
# U≈ºyj domy≈õlnych ustawie≈Ñ
```

### ≈örednia strona (100-1000 stron)
```bash
node index.js https://medium-site.com -d 5 -s 10
# Ogranicz g≈Çƒôboko≈õƒá, zapisuj co 10 URLi
```

### Du≈ºa strona (1000+ stron)
```bash
node index.js https://huge-site.com -d 3 -s 5 -c 10 -o huge.json
# P≈Çytki crawl, czƒôste zapisy, szybkie pobieranie (10 r√≥wnocze≈õnie)
# Przerwij w ka≈ºdej chwili i wzn√≥w z --resume
```

### Maksymalna szybko≈õƒá (w≈Çasny serwer)
```bash
node index.js https://mysite.com -c 20 -d 5
# 20 r√≥wnoczesnych request√≥w - bardzo szybkie!
# U≈ºywaj TYLKO dla w≈Çasnych serwer√≥w
```

### Niestabilna sieƒá
```bash
node index.js https://example.com -s 3
# Bardzo czƒôste zapisy (co 3 URLe)
```

### Testowanie
```bash
# Test p≈Çytki
node index.js https://example.com -d 2 -o test.json

# Obejrzyj wyniki...

# Crawluj g≈Çƒôbiej
node index.js https://example.com -d 5 --resume -o test.json
```

## Wyniki

### Gdzie sƒÖ zapisane?
Domy≈õlnie: `results.json` w bie≈ºƒÖcym katalogu

### Co zawiera plik?
```json
{
  "crawledAt": "2025-10-08T12:00:00.000Z",
  "startUrl": "https://example.com",
  "totalUrls": 47,
  "groupedByContentType": {
    "text/html": ["https://...", "https://..."],
    "image/jpeg": ["https://..."]
  },
  "allResults": [
    { "url": "https://...", "contentType": "text/html" }
  ]
}
```

### Jak sprawdziƒá postƒôp?
```bash
# Policz znalezione URLe
cat results.json | grep '"url"' | wc -l

# Sprawd≈∫ status
cat results.json | grep "inProgress"

# Zobacz typy contentu
cat results.json | grep "groupedByContentType" -A 20
```

## Pomoc

```bash
# Wy≈õwietl wszystkie opcje
node index.js --help

# Przyk≈Çady u≈ºycia
cat README.md

# Szczeg√≥≈Çy live save
cat LIVE-SAVE-GUIDE.md
```

## Tips & Tricks

1. **Zawsze u≈ºywaj nazwanego pliku dla du≈ºych crawli**
   ```bash
   node index.js https://site.com -o site-crawl-2024.json
   ```

2. **Monitor postƒôpu w czasie rzeczywistym**
   ```bash
   watch -n 5 "cat results.json | grep 'totalUrls'"
   ```

3. **Interrupt i Resume sƒÖ Twoimi przyjaci√≥≈Çmi**
   - Naci≈õnij Ctrl+C w ka≈ºdej chwili
   - Postƒôp jest zapisany
   - Wzn√≥w z `--resume`

4. **Filtruj po fakcie**
   ```bash
   # Crawl zapisuje WSZYSTKO
   node index.js https://site.com -o all.json
   
   # Filtruj w linii komend
   cat all.json | jq '.allResults[] | select(.contentType | contains("image"))'
   ```

5. **Testuj na ma≈Çej g≈Çƒôboko≈õci**
   ```bash
   # Najpierw test
   node index.js https://unknown-site.com -d 2
   
   # Jak wyglƒÖda dobra, id≈∫ g≈Çƒôbiej
   node index.js https://unknown-site.com --resume -d 10
   ```

## Export do CSV (Spreadsheet)

Po crawlowaniu mo≈ºesz wyeksportowaƒá wyniki do CSV dla Excel/Google Sheets:

```bash
# 1. Crawluj stronƒô
node index.js https://example.com -o mycrawl.json

# 2. Eksportuj do CSV
node json-to-csv.js mycrawl.json mycrawl.csv

# 3. Otw√≥rz w Excel lub Google Sheets
```

### Format CSV

Plik zawiera 4 kolumny:
- **DOMAIN** - domena (np. "lupine.de")
- **CONTENT_TYPE** - typ contentu (np. "text/html")
- **CURRENT_URL** - pe≈Çny URL
- **TARGET_URL** - pusta kolumna do rƒôcznego wype≈Çnienia

### Przyk≈Çad

```csv
DOMAIN,CONTENT_TYPE,CURRENT_URL,TARGET_URL
example.com,text/html,https://example.com,
example.com,text/html,https://example.com/about,
example.com,image/jpeg,https://example.com/photo.jpg,
```

### Zastosowania

1. **Planowanie redirect√≥w** - wype≈Çnij kolumnƒô TARGET_URL
2. **Audyt contentu** - analizuj typy plik√≥w
3. **Inwentaryzacja** - zobacz wszystkie assety
4. **Analiza w Excel** - pivot tables, filtrowanie

### Szybki workflow

```bash
# Crawl ‚Üí CSV ‚Üí Analysis
node index.js https://mysite.com -o site.json && \
node json-to-csv.js site.json site.csv
# Teraz otw√≥rz site.csv w Excel
```

## Gotowy do startu? üöÄ

```bash
# Crawl
node index.js https://twoja-strona.com

# Export do CSV (opcjonalnie)
node json-to-csv.js results.json
```

Mi≈Çego crawlowania! üï∑Ô∏è

